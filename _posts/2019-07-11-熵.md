---
layout: post
title: 熵（信息论）
category: 信息论
tags: [计算机, 数据]
---

----------
在信息论中，熵（英语：entropy）是接收的每条消息中包含的信息的平均量，又被称为信息熵、信源熵、平均自信息量。这里，“消息”代表来自分布或数据流中的事件、样本或特征。（熵最好理解为不确定性的量度而不是确定性的量度，因为越随机的信源的熵越大。）来自信源的另一个特征是样本的概率分布。这里的想法是，比较不可能发生的事情，当它发生了，会提供更多的信息。由于一些其他的原因，把信息（熵）定义为概率分布的对数的相反数是有道理的。事件的概率分布和每个事件的信息量构成了一个随机变量，这个随机变量的均值（即期望）就是这个分布产生的信息量的平均值（即熵）。熵的单位通常为比特，但也用Sh、nat、Hart计量，取决于定义用到对数的底。

采用概率分布的对数作为信息的量度的原因是其可加性。例如，投掷一次硬币提供了1 Sh的信息，而掷m次就为m位。更一般地，你需要用log2(n)位来表示一个可以取n个值的变量。

在1948年，克劳德·艾尔伍德·香农将热力学的熵，引入到信息论，因此它又被称为香农熵。


----------
## 1、简介
熵的概念最早起源于物理学，用于度量一个热力学系统的无序程度。在信息论里面，熵是对不确定性的测量。但是在信息世界，熵越高，则能传输越多的信息，熵越低，则意味着传输的信息越少。

英语文本数据流的熵比较低，因为英语很容易读懂，也就是说很容易被预测。即便我们不知道下一段英语文字是什么内容，但是我们能很容易地预测，比如，字母e总是比字母z多，或者qu字母组合的可能性总是超过q与任何其它字母的组合。如果未经压缩，一段英文文本的每个字母需要8个比特来编码，但是实际上英文文本的熵大概只有4.7比特。

如果压缩是无损的，即通过解压缩可以百分之百地恢复初始的消息内容，那么压缩后的消息携带的信息和未压缩的原始消息是一样的多。而压缩后的消息可以通过较少的比特传递，因此压缩消息的每个比特能携带更多的信息，也就是说压缩信息的熵更加高。熵更高意味着比较难于预测压缩消息携带的信息，原因在于压缩消息里面没有冗余，即每个比特的消息携带了一个比特的信息。香农的信息理论揭示了，任何无损压缩技术不可能让一比特的消息携带超过一比特的信息。消息的熵乘以消息的长度决定了消息可以携带多少信息。

香农的信息理论同时揭示了，任何无损压缩技术不可能缩短任何消息。根据鸽笼原理，如果有一些消息变短，则至少有一条消息变长。在实际使用中，由于我们通常只关注于压缩特定的某一类消息，所以这通常不是问题。例如英语文档和随机文字，数字照片和噪音，都是不同类型的。所以如果一个压缩算法会将某些不太可能出现的，或者非目标类型的消息变得更大，通常是无关紧要的。但是，在我们的日常使用中，如果去压缩已经压缩过的数据，仍会出现问题。例如，将一个已经是FLAC格式的音乐文件压缩为ZIP文件很难使它占用的空间变小。

### 1.1 熵的计算
如果有一枚理想的硬币，其出现正面和反面的机会相等，则抛硬币事件的熵等于其能够达到的最大值。我们无法知道下一个硬币抛掷的结果是什么，因此每一次抛硬币都是不可预测的。因此，使用一枚正常硬币进行若干次抛掷，这个事件的熵是一比特，因为结果不外乎两个——正面或者反面，可以表示为0, 1编码，而且两个结果彼此之间相互独立。若进行n次独立实验，则熵为n，因为可以用长度为n的比特流表示。但是如果一枚硬币的两面完全相同，那个这个系列抛硬币事件的熵等于零，因为结果能被准确预测。现实世界里，我们收集到的数据的熵介于上面两种情况之间。

另一个稍微复杂的例子是假设一个随机变量X，取三种可能值 \begin{smallmatrix}x_{1},x_{2},x_{3}\end{smallmatrix} 概率分别为 \begin{smallmatrix}{\frac {1}{2}},{\frac {1}{4}},{\frac {1}{4}}\end{smallmatrix}那么编码平均比特长度是：\begin{smallmatrix}{\frac {1}{2}}\times 1+{\frac {1}{4}}\times 2+{\frac {1}{4}}\times 2={\frac {3}{2}}\end{smallmatrix}其熵为3/2。

因此熵实际是对随机变量的比特量和顺次发生概率相乘再总和的数学期望。


----------
## 2、定义
依据Boltzmann's H-theorem，香农把随机变量X的熵值 Η（希腊字母Eta）定义如下，其值域为{x1, ..., xn}：

{\displaystyle \mathrm {H} (X)=\mathrm {E} [\mathrm {I} (X)]=\mathrm {E} [-\ln(\mathrm {P} (X))]} \Eta(X) = \mathrm{E}[\mathrm{I}(X)] = \mathrm{E}[-\ln(\mathrm{P}(X))]。
其中，P为X的概率质量函数（probability mass function），E为期望函数，而I(X)是X的信息量（又称为自信息）。I(X)本身是个随机变数。

当取自有限的样本时，熵的公式可以表示为：

{\displaystyle \mathrm {H} (X)=\sum _{i}{\mathrm {P} (x_{i})\,\mathrm {I} (x_{i})}=-\sum _{i}{\mathrm {P} (x_{i})\log _{b}\mathrm {P} (x_{i})},} \mathrm{H} (X)=\sum _{{i}}{{\mathrm  {P}}(x_{i})\,{\mathrm  {I}}(x_{i})}=-\sum _{{i}}{{\mathrm  {P}}(x_{i})\log _{b}{\mathrm  {P}}(x_{i})},
在这里b是对数所使用的底，通常是2,自然常数e，或是10。当b = 2，熵的单位是bit；当b = e，熵的单位是nat；而当b = 10,熵的单位是Hart。

pi = 0时，对于一些i值，对应的被加数0 logb 0的值将会是0，这与极限一致。

{\displaystyle \lim _{p\to 0+}p\log p=0} \lim_{p\to0+}p\log p = 0。
还可以定义事件 X 与 Y 分别取 xi 和 yj 时的条件熵为

{\displaystyle \mathrm {H} (X|Y)=-\sum _{i,j}p(x_{i},y_{j})\log {\frac {p(x_{i},y_{j})}{p(y_{j})}}} {\displaystyle \mathrm {H} (X|Y)=-\sum _{i,j}p(x_{i},y_{j})\log {\frac {p(x_{i},y_{j})}{p(y_{j})}}}
其中p(xi, yj)为 X = xi 且 Y = yj 时的概率。这个量应当理解为你知道Y的值前提下随机变量 X 的随机性的量。